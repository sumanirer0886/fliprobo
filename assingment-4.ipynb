{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbafd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de72f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48663d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import debugger\n",
    "import re\n",
    "\n",
    "# selenium\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "# Beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# add time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05664754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia.Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videosYou need to find following details:\n",
    "    A) Rank\n",
    "    B) Name\n",
    "    C) Artist\n",
    "    D) Upload date\n",
    "    E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb3e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\1081356034.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# first, connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d48509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for scraping the data\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "        \n",
    "# Scraping Name of the videos\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\"):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"-\")\n",
    "        \n",
    "# Scraping Artist of the videos\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\"):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\")\n",
    "        \n",
    "# Scraping Upload_Date of the videos\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\"):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append(\"-\")\n",
    "        \n",
    "# Scraping Views of the videos\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\"):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "        \n",
    "# creating DataFrame for scraped data\n",
    "Wiki = pd.DataFrame({})\n",
    "Wiki['Rank'] = Rank\n",
    "Wiki['Name'] = Name\n",
    "Wiki['Artist'] = Artist\n",
    "Wiki['Upload Date'] = Date\n",
    "Wiki['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Wiki.Name = Wiki.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e3096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Scrape the details team Indiaâ€™s internationalfixtures from bcci.tv. Url = https://www.bcci.tv/.You need to find following details:\n",
    "    A) Match title (I.e. 1stODI)\n",
    "    B) Series\n",
    "    C) Place\n",
    "    D) Date\n",
    "    E) Time\n",
    "    Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9b231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb9e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\4207499349.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb664243",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn=driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']/div/ul/li/a\")\n",
    "driver.get(btn.get_attribute(\"href\"))\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping the data\n",
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']/span[@class='u-unskewed-text fixture__format']\"):\n",
    "    Match_Title.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']/span[@class='u-unskewed-text fixture__tournament-label u-truncated']\"):\n",
    "    Series.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='fixture__description u-unskewed-text']/p/span\"):\n",
    "    Place.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fixture__datetime tablet-only']/strong[1]\"):\n",
    "    Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fd279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354a8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74652de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. Url = http://statisticstimes.com/You have to find following details:\n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)- at current prices\n",
    "    D) GSDP(19-20)- at current prices\n",
    "    E) Share(18-19)\n",
    "    F) GDP($ billion)\n",
    "    Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55393794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.guru99.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd36dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Name = []\n",
    "Description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Selenium button\n",
    "driver.find_element_by_xpath(\"//li//a[@title='Selenium']\").click()\n",
    "\n",
    "# clicking on Exception Handling button\n",
    "driver.find_element_by_xpath('//a[@title=\"Selenium Exception Handling (Common Exceptions List)\"]').click()\n",
    "\n",
    "# scraping Name\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[1]\"):\n",
    "    Name.append(i.text)\n",
    "    \n",
    "# scraping Description\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[2]\"):\n",
    "    Description.append(i.text)\n",
    "\n",
    "    \n",
    "# creating the dataframe from the scraped data\n",
    "Selenium = pd.DataFrame({})\n",
    "Selenium['Exception_Name'] = Name\n",
    "Selenium['Description'] = Description\n",
    "Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ffc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6722e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Scrape the details of trending repositories on Github.com. Url = https://github.com/You have to find the following details:\n",
    "    A) Repository title\n",
    "    B) Repository description\n",
    "    C) Contributors count\n",
    "    D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c674cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\368925491.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://statisticstimes.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Economy button\n",
    "driver.find_element_by_xpath(\"//div[@class='navbar']/div[2]/button\").click()\n",
    "\n",
    "# clicking on India\n",
    "driver.find_element_by_xpath(\"//div[@class='dropdown-content']/a[3]\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []\n",
    "\n",
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[2]\"):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[3]\"):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[4]\"):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[5]\"):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='display dataTable']/tbody/tr/td[6]\"):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")\n",
    "    \n",
    "    \n",
    "# creating DataFrame from the scraped data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share (18-19)'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d93c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b177bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\2183887372.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://github.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting explore button and clicking on it\n",
    "explore = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details\").click()\n",
    "\n",
    "# selecting trending option\n",
    "trend_url = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "# fetching urls for each repository\n",
    "repository = driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n",
    "    \n",
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//p[@class='f4 mt-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element_by_xpath(\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "    \n",
    "    \n",
    "    # scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath(\"//ul[@class= 'list-style-none']//li//span[1]\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')\n",
    "        \n",
    "        \n",
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Contributors Count'] = Contributors\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e6832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Scrape the details of Highest sellingnovels.Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/You have to find the following details:\n",
    "    A) Book name\n",
    "    B) Author name\n",
    "    C) Volumes sold\n",
    "    D) Publisher\n",
    "    E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "980f2779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\2375526105.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.billboard.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39136502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on option button\n",
    "charts=driver.find_element_by_xpath(\"//a[@class='header__main-link header__main-link--charts']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name =[]\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []\n",
    "\n",
    "# getting urls for top 100 songs\n",
    "urls = driver.find_element_by_xpath(\"//li[@class='header__submenu__list__element']//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)\n",
    "\n",
    "# scraping data of song names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\"):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\"):\n",
    "    Artist_Name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\"):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\"):\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "billiboard = pd.DataFrame({})\n",
    "billiboard['Name'] = Song_Name\n",
    "billiboard['Artist'] = Artist_Name\n",
    "billiboard['Last Week Rank'] = Last_week_rank\n",
    "billiboard['Peak Rank'] = Peak_rank\n",
    "billiboard['Weeks on board'] = Weeks_on_board\n",
    "billiboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847dc5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/You have to find the following details:\n",
    "    A) Name\n",
    "    B) Year span\n",
    "    C) Genre\n",
    "    D) Run time\n",
    "    E) Ratings\n",
    "    F) Votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "                        \n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls to navigate recruiter page\n",
    "recruiter = driver.find_element_by_xpath(\"//a[@title='Search Recruiters']\")\n",
    "page_url = recruiter.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching search button, sending keys and clicking on it\n",
    "search = driver.find_element_by_xpath(\"//div[@class='inpWrap']//input\")\n",
    "search.send_keys(\"Data Science\")\n",
    "\n",
    "btn = driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\").click()\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "\n",
    "# scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\"):\n",
    "    Name.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Designation\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\"):\n",
    "    Designation.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Company Name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]/a[2]\"):\n",
    "    Company.append(i.text)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Skills\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of Location\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]/span/small\"):\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Location.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Location.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# creating dataframe for scraped data\n",
    "Naukri = pd.DataFrame({})\n",
    "Naukri['Name'] = Name[:49]\n",
    "Naukri['Designation'] = Designation[:49]\n",
    "Naukri['Company'] = Company[:49]\n",
    "Naukri['Skills'] = Skills[:49]\n",
    "Naukri['Location'] = Location[:49]\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550100e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/You have to find the following details:\n",
    "    A) Dataset name\n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type\n",
    "    E) No of instances\n",
    "    F) No of attribute\n",
    "    G) Year\n",
    "    Note: - from the home page you have to go to the ShowAllDataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7a863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5676\\893727155.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\LENOVO\\Desktop\\Data\\chromedriver.exe\")\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "\n",
    "\n",
    "# scraping book names data\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping  data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr//td[6]\"):\n",
    "    Genre.append(i.text)    \n",
    "    \n",
    "    \n",
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4fda0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
